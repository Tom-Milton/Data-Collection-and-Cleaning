import re
import math
import requests
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup as bs
from gensim.models import Word2Vec
from nltk.tokenize import sent_tokenize, word_tokenize


def get_synonyms():
    """Returns a dictionary of synonyms for all keywords"""

    synonyms = {('targeted threat', 'targeted attack'): ' targeted-threat ',
                ('advanced persistent threat', ' apt'): ' advanced-persistent-threat ',
                ('phishing', 'whaling', 'smishing', 'vishing'): ' phishing ',
                ('dos attack', ' dos ', 'ddos', 'pdos', 'denial of service', 'denial-of-service'): ' dos-attack ',
                ('malware', 'malicious software'): ' malware ',
                ('computer virus', ' virus'): ' computer-virus ',
                ('spyware', 'adware', 'trojan'): ' spyware ',
                ('malicious bot', ' bot ', ' bots', 'botnet'): ' malicious-bot ',
                ('ransomware', 'cryptolocker', 'wannacry', 'bad rabbit'): ' ransomware ',
                ('encryption', 'encrypted', 'encrypts', ' encrypt ', 'encrypting'): ' encryption '}

    return synonyms


def relevance_filter(article_link):
    """Checks article links to see if they are relevant to our keywords
    (i.e. they contain any keyword or one of their synonyms)"""

    # Download webpage for current article
    webpage = requests.get(article_link)
    soup = bs(webpage.content, 'html.parser')

    # Extract contents as string
    contents = soup.find_all(class_='e5tfeyi2')  # This code is linked with the article body paragraph
    contents_text = str([i.text.lower() for i in contents])

    # Retrieve list of synonyms for all keywords
    synonyms = list(sum(get_synonyms().keys(), ()))

    # Returns true or false if any synonym is or isn't in the article
    if any(synonym in contents_text for synonym in synonyms):
        return True
    else:
        return False


def top_100_articles(keyword):
    """Retrieves links for top the top 100 articles of a given given keyword, using the BBC news search feature"""

    # Initialize lists
    article_links = []

    # Iterates through all max 29 search pages
    # If there are less than 29 search pages for a given keyword BBC news will wrap back to the first page
    # This is not a problem as we have already checked most links so it will be quick to iterate through the pages
    for current_page in range(1, 30):

        # Download the webpage of search queries for each keyword and extract the articles
        webpage = requests.get('https://www.bbc.co.uk/search?q=' + keyword + '&page=' + str(current_page))
        soup = bs(webpage.content, 'html.parser')
        articles = soup.find_all(class_='e1f5wbog5')  # This code is linked with all article hyperlinks

        # Extract new relevant news article links and adds them to article_links
        links = [i['href'] for i in articles]  # Extracts the hyperlinks
        links = [i for i in links if i not in article_links]  # Keeps non-checked links
        links = [i for i in links if '/news/' in i]  # Keeps news articles
        links = [i for i in links if relevance_filter(i)]  # Keeps relevant articles
        article_links += links

        print('page', current_page, 'done')

    print(len(article_links[:100]), 'relevant articles retrieved', '\n')

    return article_links[:100]


def save_article_contents(keyword, article_urls):
    """Saves all article contents from the links generated by top_100_articles to a text file"""

    # Creates file for storing article contents
    file_name = keyword + ' bbc.txt'
    with open(file_name, 'w+', encoding='utf-8') as f:

        for url in article_urls:

            # Download webpage contents for current article
            webpage = requests.get(url)
            soup = bs(webpage.content, 'html.parser')
            contents = soup.find_all(class_='e5tfeyi2')  # This code is linked with the article body paragraph

            # Iterate through contents and write to file
            for paragraph in contents:
                f.write(paragraph.text.lower() + ' ')  # Adds space to prevent paragraphs being joined together

            print(url + ' done')

    return file_name


def wiki_articles(keyword):
    """Downloads wikipedia article relevant to the given keyword"""

    # Creates file and url for given keyword
    file_name = keyword + ' wiki.txt'
    url = 'https://en.wikipedia.org/wiki/' + keyword.replace(' ', '_')

    with open(file_name, 'w+', encoding='utf-8') as f:

        # Download webpage for search article
        webpage = requests.get(url)
        soup = bs(webpage.content, 'html.parser')
        contents = soup.select('p')

        # Iterate through contents and write to file
        for paragraph in contents:
            f.write(paragraph.text.lower() + ' ')  # Adds space to prevent paragraphs being joined together


def join_and_count(keywords, files, synonyms=True):
    """Joins the text files to create a master file containing the contents of all files.
    Then it counts the occurrences of each keyword (and its synonyms) in this master file """

    # Hyphenates two-word keywords so they can be interpreted as one
    keyword_corrections = {(i,): ' ' + i.replace(' ', '-') + ' ' for i in keywords}
    if synonyms:  # Retrieves synonyms
        keyword_corrections = get_synonyms()

    # Creates master file
    with open('master file.txt', 'w+', encoding='utf-8') as outfile:
        for filename in files:
            with open(filename, encoding='utf-8') as infile:
                for line in infile:

                    # Replace two word keywords and synonyms with our corrected keywords
                    for old, new in keyword_corrections.items():
                        for i in old:
                            line = line.replace(i, new)

                    # Appends to master files
                    outfile.write(line)

    # Gets keyword occurrence counts
    with open('master file.txt', encoding='utf-8') as f:
        f = f.read()

        # Counts exact occurrences of keyword including punctuation e.g. ' cup.' is accepted but ' cupboard ' is not accept if 'cup' was a keyword
        keyword_counts = [sum(1 for _ in re.finditer(r'\b%s\b' % re.escape(i.replace(' ', '-')), f)) for i in keywords]

    return keyword_counts


def train_model():
    """Train word2vec model on the master file"""

    # Initialise master list
    tokenized_master = []

    # Open master text file
    with open('master file.txt', encoding='utf-8') as f:
        f = f.read()

        # Tokenize text file (splits into a list of lists of sentences where each element is a word)
        sentences = [word_tokenize(i) for i in sent_tokenize(str(f))]
        sentences = [[word.lower() for word in sentence] for sentence in sentences]

        # Add tokenized text file to master list
        tokenized_master += sentences

    # Trains and saves model
    model = Word2Vec(tokenized_master, iter=100)
    model.save('Word2Vec.model')


def Word2Vec_distance(keywords, df):
    """Calculate the semantic distance via Word2Vec. 0 is least similar, 1 is most similar"""

    # Loads Word2Vec model
    model = Word2Vec.load('Word2Vec.model')

    # Iterate through pairs of keywords and works out the semantic distances
    for x in keywords:
        for y in keywords:

            if x == y:  # Same keyword means exact similarity
                distance = np.nan

            else:
                try:
                    distance = model.wv.similarity(x.replace(' ', '-'), y.replace(' ', '-'))
                except KeyError:  # Except if the number of keyword occurrences is 0
                    distance = 0

            # Adds distance to dataframe
            df[x][y] = distance

    return df


def get_num_results(keyword):
    """Retrieves the number of google results for the given keyword"""

    # Creates url and user-agent for given keyword
    url = 'https://www.google.com/search?q=' + keyword.replace(' ', '+')
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36"}

    # Loads webpage
    webpage = requests.get(url, headers=headers)
    soup = bs(webpage.content, 'html.parser')

    # Extracts the number of search results
    results_str = soup.find('div', {'id': 'result-stats'})
    results_int = int(results_str.text.replace(",", "").split()[1])

    print(url + ' done')

    return results_int


def NGD_distance(keywords, df):
    """Calculates semantic distance via Normalized Google Distance. 1 is least similar, 0 is most similar"""

    # Proxy for total number of search results
    N = math.log(25270000000, 2)

    # Retrieves the number of search results for each keyword
    keyword_results = {i: math.log(get_num_results(i), 2) for i in keywords}

    # Iterate through pairs of keywords and works out the semantic distances
    for x in keywords:
        for y in keywords:

            if x == y:  # Same keyword means exact similarity
                distance = np.nan

            else:
                # Calculates number of search results for the two keywords together
                xy_results = math.log(get_num_results(x + " " + y), 2)
                # Calculates the distance using the given formula
                distance = (max(keyword_results[x], keyword_results[y]) - xy_results) / (
                        N - min(keyword_results[x], keyword_results[y]))

            # Adds distance to dataframe
            df[x][y] = distance

    return df


def plot_counts(tidy_df):
    """Plots the number of occurrences of each keyword"""

    fig, ax = plt.subplots(figsize=(10, 6))
    sns.barplot(x='keywords', y='value', hue='variable', data=tidy_df, palette='Greens', ax=ax)
    ax.set_title('Keyword Occurrence Counts', fontsize=14)
    ax.set_ylabel('number of keyword occurrences')
    ax.legend(loc=2, title=None)

    # Adds values of columns to the top of their relative bars
    for p in ax.patches:
        ax.annotate(int(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height() + 28), ha='center', va='top',
                    color='black', size=10)

    plt.tight_layout()
    plt.savefig('Keyword Occurrence Counts.png')


def plot_difference(difference_array, keywords):
    """Plot the number of missed keyword occurrences caught"""

    # Colours bars based on height
    pal = sns.color_palette('Reds', len(difference_array))
    rank = difference_array.argsort().argsort()  # Use argsort twice, first to obtain the order of the array, then to obtain ranking

    # Sets tick labels for keywords such that each word is on a new line
    tick_labels = [i.replace(' ', '\n') for i in keywords]

    fig, ax = plt.subplots(figsize=(10, 6))
    sns.barplot(x=tick_labels, y=difference_array, palette=np.array(pal[::1])[rank], ax=ax)
    ax.set_title('Number of Keywords Caught', fontsize=14)
    ax.set_ylabel('number of keywords caught')
    ax.set_xlabel('keywords')

    # Adds values of columns to the top of relative bars
    for p in ax.patches:
        ax.annotate(int(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height() + 15), ha='center', va='top',
                    color='black', size=10)

    plt.tight_layout()
    plt.savefig('Number of Keywords Caught.png')


def heatmap(excel_file, title):
    """Visualise the semantic distance results as a heatmap"""

    # Loads dataframe from given excel file
    df = pd.read_excel(excel_file, index_col=0)

    # Sets tick labels for keywords such that each word is on a new line
    tick_labels = [i.replace(' ', '\n') for i in df.columns]

    # Plots Semantic distance heatmap
    fig, ax = plt.subplots(figsize=(10, 6))
    title = title + ' Semantic Distance Heatmap'
    ax.set_title(title, fontsize=14)
    sns.heatmap(df, cmap='YlGnBu', annot=True, xticklabels=tick_labels, yticklabels=tick_labels, ax=ax)
    ax.set_xlabel(''), ax.set_ylabel('')

    plt.tight_layout()
    plt.savefig(title + '.png')


def main(redownload_articles=False, recompute_NGD=True, recompute_Word2Vec=False):

    # Extract list of keywords from 'keywords.xlsx'
    keywords_df = pd.read_excel('keywords.xlsx', index_col=0)
    keywords = keywords_df.columns

    # Extract BBC news and wiki article contents and saves to files (Problem 1 & Problem 2)
    if redownload_articles:
        for keyword in keywords:
            print('\033[95m', 'keyword:', keyword, '\033[0m')
            article_urls = top_100_articles(keyword)
            save_article_contents(keyword, article_urls)
            wiki_articles(keyword)
            print('')

    # Gets the relative keyword counts for bbc and wiki articles
    bbc_files = [i + ' bbc.txt' for i in keywords]
    wiki_files = [i + ' wiki.txt' for i in keywords]
    bbc_keyword_counts = join_and_count(keywords, bbc_files)
    bbc_and_wiki_keyword_counts = join_and_count(keywords, bbc_files + wiki_files)

    # Turns the counts into a dataframe
    keyword_counts = pd.DataFrame({
        'keywords': [i.replace(' ', '\n') for i in keywords],
        'bbc and wiki': bbc_and_wiki_keyword_counts,
        'bbc': bbc_keyword_counts
    })

    tidy = keyword_counts.melt(id_vars='keywords')  # Melts df so it is in correct format for plotting
    plot_counts(tidy)  # Plots improvement from additionally using wiki articles

    # Gets the relative keyword counts for catching and not catching synonyms
    no_synonyms = join_and_count(keywords, bbc_files + wiki_files, False)
    synonyms = join_and_count(keywords, bbc_files + wiki_files, True)

    # Calculates and plots the improvement from catching synonyms
    difference_array = np.array(synonyms) - np.array(no_synonyms)
    plot_difference(difference_array, keywords)

    if recompute_Word2Vec:  # (Problem 3)
        print('\033[95m', 'performing Word2Vec', '\033[0m')
        train_model()
        df = Word2Vec_distance(keywords, keywords_df.copy())
        df = (df - df.min().min()) / (df.max().max() - df.min().min())  # Normalise df
        df.to_excel('Word2Vec distance.xlsx', index=True)
        print('')

    if recompute_NGD:  # (Problem 3)
        print('\033[95m', 'performing NGD', '\033[0m')
        df = NGD_distance(keywords, keywords_df.copy())
        df = (df - df.min().min()) / (df.max().max() - df.min().min())  # Normalise df
        df = 1 - df  # Flip df so 1 is most similar and 0 is least
        df.to_excel('NGD distance.xlsx', index=True)
        print('')

    # Calculates average semantic distances as final result
    final_df = (pd.read_excel('Word2Vec distance.xlsx', index_col=0) + pd.read_excel('NGD distance.xlsx', index_col=0)) / 2
    # Normalise df (1 call only returns a list of the min/max of each features)
    final_df = (final_df - final_df.min().min()) / (final_df.max().max() - final_df.min().min())
    # Save final result to excel
    final_df.to_excel('distance.xlsx', index=True)

    # Plots heatmaps
    heatmap('Word2Vec distance.xlsx', 'Word2Vec')
    heatmap('NGD distance.xlsx', 'NGD')
    heatmap('distance.xlsx', 'Final')
    plt.show()


main(redownload_articles=False, recompute_NGD=False, recompute_Word2Vec=False)
